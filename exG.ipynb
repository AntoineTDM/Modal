{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KG5BC2BdcUf"
      },
      "source": [
        "# MAP 435 - TP à trous à rendre pour le mardi 9 mai 23h59\n",
        "\n",
        "Le but de cet exercice est de compléter les fonctions manquantes en remplaçant les parties `# YOUR CODE HERE` par votre code.\n",
        "\n",
        "**Attention** la correction des notebooks se faisant de manière automatique :\n",
        "- il faut que vous supprimiez la ligne `raise NotImplementedError()` ;\n",
        "- les seules cellules que vous devez modifier sont celles comportant la mention `# YOUR CODE HERE` ;\n",
        "- vous ne devez pas modifier le nom du fichier faute de quoi votre devoir ne sera pas corrigé.\n",
        "\n",
        "Merci d'inscrire votre nom dans la cellule ci-dessous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G66mmjQqdcUj"
      },
      "outputs": [],
      "source": [
        "NOM = \"MILLET\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgniJ2uodcUl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "df534836b628141d5dcd61fdddbb3b68",
          "grade": false,
          "grade_id": "cell-b6c4d6360018071f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "V4kOzWMSdcUo"
      },
      "source": [
        "# Algorithme des moindres carrés non-linéaire\n",
        "\n",
        "Dans ce notebook nous allons utiliser un algorithme qui permet de minimiser des fonctions qui s'écrivent comme des sommes de carrés. Une grande classe de telles fonctionnelles s'obtient à partir de problèmes de recherche de paramètres d'un modèle à partir d'un critère de type moindres carrés. \n",
        "\n",
        "Considérons une fonction sous la forme\n",
        "$$f(x) = \\sum_{j=1}^m r_j(x)^2.$$\n",
        "La matrice Jacobienne associée est\n",
        "$$ J(x) = \\begin{pmatrix}\n",
        "\\frac{\\partial r_1}{\\partial x_1}& \\cdots & \\frac{\\partial r_1}{\\partial x_n} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{\\partial r_m}{\\partial x_1}& \\cdots & \\frac{\\partial r_m}{\\partial x_n}\n",
        "\\end{pmatrix}.$$\n",
        "Il est immédiat de voir que le gradient de $f$ s'écrit sous la forme \n",
        "$$ \\nabla f(x) = 2(J(x))^T r$$\n",
        "avec $r = (r_1,...,r_m)^T$. \n",
        "\n",
        "La méthode que l'on utilise, appelée méthode de Gauss-Newton, consiste à construire la suite $x_i$ définie par :\n",
        "\n",
        "\\begin{equation}\n",
        "\\left\\{\n",
        "\\begin{aligned}\n",
        "&x_0 \\in \\mathbb R^N \\text{ quelconque} \\\\\n",
        "& x_{i+1} = x_i-\\gamma_i (J(x_i)^T J(x_i))^{-1} J^T(x_i) r(x_i).\n",
        "\\end{aligned}\n",
        "\\right.\n",
        "\\end{equation}\n",
        "\n",
        "Cette méthode peut être vue comme une méthode de Newton incomplète, car $J(x_i)^T J(x_i)$ représente seulement le premier terme dans le calcul de la Hessienne. \n",
        "\n",
        "Le pas de descente $\\gamma_i$ est à choisir avec un algorithme de recherche de pas. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d19ac508cb5356e61e727b52fe9aebac",
          "grade": false,
          "grade_id": "cell-77a8b925a56c315e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Q_8rrfQLdcUr"
      },
      "source": [
        "# Chargement des librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "47942f600dc034d51c7fc96590b3bd96",
          "grade": false,
          "grade_id": "cell-c96f44ab61435905",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "4-HJJqUBdcUt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.testing as npt\n",
        "\n",
        "import pylab as pl\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib notebook\n",
        "\n",
        "n = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2QuCK6mdcUu"
      },
      "source": [
        "# La règle de Goldstein-Price pour la recherche du pas\n",
        "\n",
        "Les algorithmes de recherche de pas assurent que la fonction décroît de manière suffisante et que le pas n'est pas trop petit, pour accélerer la convergence. \n",
        "\n",
        "Soit $x$ un point et $d$ une direction de descente, i.e. $\\nabla f(x) \\cdot d<0$. Considérons la fonction de mérite $q(t) = f(x+td)$. On peut voir que, par définition de la direction de descente $d$, on a $q'(0) = d\\cdot \\nabla f(x)<0$.\n",
        "\n",
        "\n",
        "Considérons $m_1<m_2 \\in (0,1)$, $m_1<1/2$. Pour un pas de descente $t>0$, nous avons les trois décisions suivantes :\n",
        "\n",
        "**(a) Acceptation du pas** :\n",
        "$\\displaystyle m_2 q'(0) \\leq \\frac{q(t)-q(0)}{t}\\leq m_1 q'(0)$.\n",
        "\n",
        "**(b) Pas trop grand** : $\\displaystyle m_1 q'(0)<\\frac{q(t)-q(0)}{t}$.\n",
        "\n",
        "**(c) Pas trop petit** : $\\displaystyle \\frac{q(t)-q(0)}{t}< m_2q'(0)$.\n",
        "\n",
        "Ces décisions sont placées dans une boucle et à chaque itération un intervalle $[t_l,t_r]$ contenant un pas acceptable est mis à jour. Si la condition (a) est verifiée, le pas actuel convient. \n",
        "\n",
        "Sinon, tant qu'une borne supérieure n'a pas été trouvée, on augmente le pas (en le multipliant par 2, par exemple). Si $t_r$ existe, alors on choisit un nouveau pas dans $[t_l,t_r]$, par exemple $t = (t_l+t_r)/2$. Si (b) est verifiée, alors $t_r=t$ (nouvelle borne supérieure). Si (c) est verifiée alors $t_l=t$ (nouvelle borne inférieure).  \n",
        "\n",
        "Pour des raisons de simplicité les deux conditions (a) et (b) (la condition (c) étant la complémentaire) seront à implémenter en utilisant des fonctions qui prennent comme entrées les variables $t,q(0),q(t),q'(0),m_1,m_2$ notées respectivement `t,q0,qt,qp0,qpt,m1,m2` et qui renvoient `True` si la condition est vérifiée.\n",
        "\n",
        "**Q1)** Complétez la fonction ci-dessous qui correspond à la condition (a)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "05edafa2629c464186da02b84e90b96b",
          "grade": false,
          "grade_id": "cell-026a2e44ca86cb2a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "awzdOxUHdcUw"
      },
      "outputs": [],
      "source": [
        "def aGP(t,q0,qt,qp0,m1,m2):\n",
        "    # définir un booléen qui renvoit True si la \n",
        "    # condition (a) est vérifiée\n",
        "    \n",
        "    if m2*qp0<= (qt-q0)/t <= m1*qp0:\n",
        "      return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvGAWRPHdcUx"
      },
      "source": [
        "**Q2)** Complétez la fonction ci-dessous qui correspond à la condition (b)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f0d165b8e5f76b6ceaa9136a4756ce62",
          "grade": false,
          "grade_id": "cell-978435bac539310e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "u1EqxFtrdcUy"
      },
      "outputs": [],
      "source": [
        "def bGP(t,q0,qt,qp0,m1,m2):\n",
        "    # définir un booléen qui renvoit True si la \n",
        "    # condition (b) est verifiée\n",
        "    \n",
        "    if (qt-q0)/t > m1*qp0:\n",
        "      return True\n",
        "    return False\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iKZ4Gf8dcUz"
      },
      "source": [
        "Les fonctions ci-dessous (à ne pas modifier) effectuent la recherche d'un pas qui respecte les régles de Goldstein-Price. Elles utilisent les fonctions de décision implementées dans les cellules précédentes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "jEHi51WbdcU0"
      },
      "outputs": [],
      "source": [
        "def GPDecision(t,q0,qt,qp0,qpt,m1,m2):\n",
        "    # function which returns \n",
        "    # 1 if the step can be accepted\n",
        "    # 2 if the step is too big\n",
        "    # 3 if the step is too small\n",
        "    if aGP(t,q0,qt,qp0,m1,m2):\n",
        "        return 1\n",
        "    elif bGP(t,q0,qt,qp0,m1,m2):\n",
        "        return 2\n",
        "    else: \n",
        "        return 3\n",
        "    \n",
        "# linesearch\n",
        "def GP_linesearch(x0,d,fun,gradf,m1,m2):\n",
        "    # initialisation \n",
        "    # \n",
        "    t = 1\n",
        "    tl = 0 # lower bound\n",
        "    tr = 0 # upper bound\n",
        "    while (1==1):\n",
        "        #print(tl,\" \",tr)\n",
        "        q0 = fun(x0)  # q(0)\n",
        "        qt = fun(x0+t*d) #q(t)\n",
        "        # gradient direction\n",
        "        gd = gradf(x0)\n",
        "        qp0 = np.dot(gd,d) # q'(0)\n",
        "        qpt = np.dot(gradf(x0+t*d),d) # q'(t)\n",
        "            #print(tl,\" \",tr)\n",
        "        dec = GPDecision(t,q0,qt,qp0,qpt,m1,m2) \n",
        "        #print(\"t=\",t,\"q0=\",q0,\"qt=\",qt,\"qp0=\",qp0,\"m1=\",m1,\"m2=\",m2)\n",
        "        #print(dec)   \n",
        "        if dec==1:\n",
        "            step=t   # we found a good step\n",
        "            break\n",
        "        elif dec==2:      \n",
        "            tr = t  # step too big, update upper bound\n",
        "        else:\n",
        "            tl = t  # step too small, update lower bound\n",
        "        \n",
        "        # if no upper bound exists, pick a bigger t\n",
        "        if(tr==0):\n",
        "            t = 2*tl\n",
        "        else:  \n",
        "            # pick the midpoint\n",
        "            t = 0.5*(tl+tr)\n",
        "        \n",
        "        if(abs(tr-tl))<1e-15:\n",
        "            print(\"Something is wrong!\")\n",
        "            break\n",
        "    return step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF6w6SKldcU1"
      },
      "source": [
        "# La fonction de Rosenbrock\n",
        "\n",
        "On considère pour $n \\geq 2$ la fonction $f:\\Bbb{R}^n \\to \\Bbb{R}$ définie par\n",
        "$$f(x) = \\sum_{i=1}^{n-1} 100(x_{i+1}-x_i^2)^2+(1-x_i)^2$$\n",
        "admettant un unique minimiseur $x^* = (1,1,...,1)$ sur $\\Bbb{R}^n$.\n",
        "\n",
        "Cette fonction est souvent utilisée comme test pour des algorithmes d'optimisation. Son mauvais contitionnement fait que les algorithmes basés sur une descente de gradient ont une convergence trés lente vers le minimum.\n",
        "\n",
        "En vue de la forme de l'algorithme de Gauss-Newton nous allons travailler avec les $2n-2$ fonctions de moindres carrés $r_i$, plutôt qu'avec la fonction $f$ :\n",
        "$$ r_{i}(x) = 10(x_{i+1}-x_i^2),\\ \\ r_{i+(n-1)}(x) = 1-x_i, 1\\leq i \\leq n-1.$$\n",
        " \n",
        " \n",
        "**Q3)** Complétez le code ci-dessous qui pour un vecteur $x$ donné renvoit la fonction \n",
        "$r(x)=\\begin{pmatrix} r_1(x) \\\\ r_2(x) \\\\ ... \\\\ r_{2n-2}(x) \\end{pmatrix}.$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c89b37b6d82232bb8099ea4052ce8e69",
          "grade": false,
          "grade_id": "cell-451a4bff967baba0",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "kDXdgrW3dcU4"
      },
      "outputs": [],
      "source": [
        "def r(x):\n",
        "    \"\"\" Compute the Rosenbrock function.\n",
        "    Only use the variables in the function signature.\n",
        "    \"\"\"\n",
        "    #n=len(x) en fait n=100 (enonce)\n",
        "    r=[]\n",
        "    for i in range(n-1):\n",
        "      r.append(10*(x[i+1]-x[i]**2))\n",
        "    for i in range(n-1):\n",
        "      r.append(1-x[i])\n",
        "    return r\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMo4zvhPdcU6"
      },
      "source": [
        "**Q4)** Complétez le code ci-dessous qui pour un vecteur $x$ donné renvoit la matrice jacobienne associée à la fonction $r$ précédemment définie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "56b33da06487c68ff5dcc43682d57360",
          "grade": false,
          "grade_id": "cell-3da6dc38c020a72f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "_QxFoi-9dcU9"
      },
      "outputs": [],
      "source": [
        "def Jac(x):\n",
        "    \"\"\" Compute the Jacobian function.\n",
        "    La Jacobienne contient sur chaque ligne les derivees partielles de r_i par rapport a chaque variable\n",
        "    Only use the variables in the function signature.\n",
        "    \"\"\"\n",
        "    Jac=[]\n",
        "    for i in range(n-1):\n",
        "      L=np.zeros(n)\n",
        "      L[i]=-20*x[i]\n",
        "      L[i+1]=10\n",
        "      Jac.append(L)\n",
        "    for i in range(n-1):\n",
        "      L=np.zeros(n)\n",
        "      L[i]=-1\n",
        "      Jac.append(L)\n",
        "    return Jac\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy2cQ0rVdcU_"
      },
      "source": [
        "A l'aide des fonctions $r_i$ et de leurs dérivées, on calcule maintenant la valeur de la fonction objectif et de son gradient.\n",
        "\n",
        "**Q5)** Completez la fonction ci-dessous pour calculer la fonction objectif $J(x) = \\sum_{i=1}^{2n-2} r_i(x)^2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2f76ad0c2c1bf499bcc23daaa05c3cf2",
          "grade": false,
          "grade_id": "cell-28291fe0bb4264ce",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Fx59sApbdcVB"
      },
      "outputs": [],
      "source": [
        "# objective function\n",
        "def J(x):\n",
        "    \"\"\" Compute the Objective function.\n",
        "    Only use the variables in the function signature.\n",
        "    \"\"\"\n",
        "    return np.dot(r(x),r(x))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5FXpjcCdcVE"
      },
      "source": [
        "**Q6)** Completez la fonction ci-dessous qui calcule le gradient de la fonction objectif $\\nabla J(x) = 2Dr(x)^T r(x)$ où $Dr(x)$ est la matrice Jacobienne de $r$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "59814b7ccd5a7b92a26bb62b3251d38f",
          "grade": false,
          "grade_id": "cell-0b67a30c4a965304",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ZYAGsv5GdcVF"
      },
      "outputs": [],
      "source": [
        "# gradient        \n",
        "def GradJ(x):\n",
        "    \"\"\" Compute the Objective function.\n",
        "    Only use the variables in the function signature.\n",
        "    \"\"\"\n",
        "    Dr=Jac(x)\n",
        "    rx=r(x)\n",
        "    return 2*np.dot(np.transpose(Dr),rx)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1635bfac5b0cb36c210d497f4081946c",
          "grade": false,
          "grade_id": "cell-b5ae4a40f331be10",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "FjfgErBwdcVG"
      },
      "source": [
        "Le but de cette question est, pour $x$ donné, de calculer la direction de descente de Gauss-Newton donnée par :\n",
        "$$d(x)=-(J(x)^T J(x))^{-1} J^T(x)r(x).$$\n",
        "On rappelle qu'un calcul du type $A^{-1}b$ implique la résolution du systeme linéaire $Ax=b$. La routine `np.linalg.solve` permet de résoudre un tel système linéaire. \n",
        "\n",
        "**Q7)** Complétez le code ci-dessous ayant pour arguments d'entrée la matrice Jacobienne $J(x)$ notée `Jr` et le vecteur $r(x)$ noté `vec_r` et qui renverra la direction de descente correspondante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d600597ee33eaa898565365e3102108f",
          "grade": false,
          "grade_id": "cell-8da0fcba280c20df",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "7Agyk0SfdcVH"
      },
      "outputs": [],
      "source": [
        "def GNdir(Jr,vec_r):\n",
        "  A=np.dot(np.transpose(Jr),Jr)\n",
        "  b=np.dot(np.transpose(Jr),vec_r)\n",
        "  return -np.linalg.solve(A,b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "62a421349954dd2939322fbc6349172d",
          "grade": false,
          "grade_id": "cell-b436b00ef1bdd2f2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "0esk5gSGdcVI"
      },
      "outputs": [],
      "source": [
        "# Choix de l'initialisation et de divers paramètres\n",
        "\n",
        "x0 = np.zeros(n)\n",
        "\n",
        "# une initialisation \"aleatoire\"\n",
        "for i in range(0,n):\n",
        "    x0[i] = 2*(-1)**i+0.2**i\n",
        "\n",
        "analytic = np.ones(n)\n",
        "\n",
        "m1 = 0.1\n",
        "m2 = 0.9\n",
        "Tol = 1e-16\n",
        "Maxiter = 300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ad2b54a52a79381b9d567a8103f81b6c",
          "grade": false,
          "grade_id": "cell-43b76381296a3aee",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "UakiLzfddcVJ"
      },
      "source": [
        "# Choix de la direction de descente\n",
        "\n",
        "Dans la cellule suivante on implémente le choix de la direction de descente. Pour une comparaison facile, on proposera soit la direction opposée au gradient : $d=-\\nabla f(x)$ (choix classique), soit la direction proposée par l'algorithme de Gauss-Newton. La variable `v` permettra de changer facilement entre les deux choix proposés. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "45a445a1f68ee2b998164de7677f0add",
          "grade": false,
          "grade_id": "cell-bfcd717125842823",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Tc82KNbSdcVK"
      },
      "outputs": [],
      "source": [
        "def Choix(v,fun,gradf,x0):\n",
        "    if(v==1): # descente gradient\n",
        "        d = -gradf(x0)\n",
        "    else:# Gauss-Newton\n",
        "        d = GNdir(Jac(x0),r(x0))\n",
        "    return d\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1b554e4352ee8b2238bb6c38407cb38f",
          "grade": false,
          "grade_id": "cell-c178618f378a6b15",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "xQU0bMl1dcVK"
      },
      "source": [
        "# Descente de gradient avec un line search\n",
        "\n",
        "La cellule suivante (à ne pas modifier) implémente l'algorithme générique de descente de gradient, avec une eventuelle possibilité de modifier la direction de descente. A l'intérieur de la boucle d'optimisation on pourra observer la présence de la routine de recherche d'un pas qui satisfait les conditions de Goldstein-Price. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6c88744629696e04b505890f3d8238a7",
          "grade": false,
          "grade_id": "cell-58a077e23958c07b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "GEFHPwdpdcVL"
      },
      "outputs": [],
      "source": [
        "def GDlinesearch(fun,gradf,x0,Tol,maxiter,m1,m2,v=1):\n",
        "    print(\"=================================================================================\")\n",
        "    if(v==1):\n",
        "        print(\"Optimisation utilisant une descente de gradient avec recherche du pas Goldstein-Price\")\n",
        "    else:\n",
        "        print(\"Optimisation utilisant Gauss-Newton avec recherche du pas Goldstein-Price\")\n",
        "    phist = []\n",
        "    vhist = []\n",
        "    ghist = []\n",
        "    phist.append(x0)            # Create an array which holds the optimization history\n",
        " \n",
        "    val = fun(x0)\n",
        "    gd   = gradf(x0)\n",
        "    vhist.append(val)\n",
        "    ghist.append(gd)\n",
        "    iter = 0\n",
        "\n",
        "    while abs(np.linalg.norm(gd))>=Tol: \n",
        "        iter=iter+1\n",
        "        # recherche du pas\n",
        "        d = Choix(v,fun,gradf,x0)\n",
        "        #rint(d)\n",
        "        #print(np.dot(d,gd))\n",
        "        #print(\"This is where it goes wrong\")\n",
        "        step = GP_linesearch(x0,d,fun,gradf,m1,m2)\n",
        "        #print(\"issue fixed!\")\n",
        "            \n",
        "        x0 = x0+step*(d)\n",
        "        val = fun(x0)\n",
        "        gd   = gradf(x0)\n",
        "        if(iter%50==0):\n",
        "            print(\"Iter: \",iter,\"| Val: \",val,\"| Step: \",step,\" Grad: \",np.linalg.norm(gd))\n",
        "        phist.append(x0)\n",
        "        vhist.append(val)\n",
        "        ghist.append(d)\n",
        "        if(iter>maxiter):\n",
        "            print('Maximum number of iterations reached!')\n",
        "            break\n",
        "        #if(abs(val-prevval)<Tol):\n",
        "            #print('Function does not decrease enough!')\n",
        "            #break\n",
        "        prevval = val\n",
        "    if(np.linalg.norm(d)<Tol): \n",
        "        print('Algorithm converged!')\n",
        "    print('')\n",
        "    print('Final output:')\n",
        "    print(\"Iter: \",iter,\"| Val: \",val,\"| Step: \",step,\" Grad: \",np.linalg.norm(gd))\n",
        "    print(\"=================================================================================\")\n",
        "\n",
        "    return phist,vhist,ghist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9a73ed00889a395e88d3baea760446d1",
          "grade": false,
          "grade_id": "cell-1bc6012269e09568",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "skN6OH3ZdcVM"
      },
      "source": [
        "# Comparaison : Gauss-Newton vs Descente de Gradient classique\n",
        "\n",
        "Dans la cellule suivante on pourra observer l'amélioration de la vitesse de convergence obtenue en utilisant la direction donnée par l'algorithme de Gauss-Newton. Pour la fonction Rosenbrock en 2D l'algorithme de gradient, même ayant de bonnes propriétés théoriques de convergence a du mal à converger rapidement vers l'optimum. Ceci est dû au mauvais conditionnement de la fonction autour de l'optimum $(1,1)$. L'algorithme de Gauss-Newton permet d'obtenir une convergence très rapide, en quelques itérations seulement, ceci en utilisant seulement des informations sur les dérivées premières.  \n",
        "\n",
        "Le bon comportement de l'algorithme de Gauss Newton est justifié par l'exploitation efficace de la structure de la fonction objectif (somme de carrés). En plus, pour des fonctions $r_i$ qui ne sont pas trop oscillatoires, autour d'un minimum, la direction donnée par Gauss-Newton va s'approcher de la direction de descente de l'algorithme de Newton, avec des propriétés de convergence améliorées (voir le cours). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "df81b75722227fd6b441b24412b1bb3d",
          "grade": false,
          "grade_id": "cell-84efaa48d6aea01b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY0mkBZhdcVN",
        "outputId": "2466499e-fd80-4616-b430-627106ea1b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================================\n",
            "Optimisation utilisant une descente de gradient avec recherche du pas Goldstein-Price\n",
            "Iter:  50 | Val:  0.013131034972966029 | Step:  0.0009765625  Grad:  1.1216112743702615\n",
            "Iter:  100 | Val:  0.011183335030591013 | Step:  0.0009765625  Grad:  0.19030592231643162\n",
            "Iter:  150 | Val:  0.010436630757118929 | Step:  0.001953125  Grad:  0.3026932017294806\n",
            "Iter:  200 | Val:  0.009725878147145654 | Step:  0.0009765625  Grad:  0.19027762258938016\n",
            "Iter:  250 | Val:  0.00908684578310099 | Step:  0.001953125  Grad:  0.3113227105450119\n",
            "Iter:  300 | Val:  0.008471050302016104 | Step:  0.0009765625  Grad:  0.1923175730324263\n",
            "Maximum number of iterations reached!\n",
            "\n",
            "Final output:\n",
            "Iter:  301 | Val:  0.008458379399140495 | Step:  0.0009765625  Grad:  0.15943164591773656\n",
            "=================================================================================\n",
            "Number of iterations:  302\n",
            "Final position:  [0.99999975 1.00000062 0.99999901 1.00000135 0.99999828 1.00000208\n",
            " 0.99999757 1.00000278 0.99999687 1.00000347 0.9999962  1.00000412\n",
            " 0.99999557 1.00000473 0.99999497 1.00000531 0.99999441 1.00000585\n",
            " 0.9999939  1.00000634 0.99999343 1.00000678 0.99999301 1.00000718\n",
            " 0.99999263 1.00000753 0.99999231 1.00000784 0.99999202 1.00000811\n",
            " 0.99999177 1.00000833 0.99999157 1.00000852 0.99999139 1.00000868\n",
            " 0.99999125 1.00000881 0.99999114 1.00000891 0.99999105 1.00000899\n",
            " 0.99999098 1.00000905 0.99999093 1.00000909 0.99999089 1.00000912\n",
            " 0.99999087 1.00000914 0.99999086 1.00000914 0.99999086 1.00000913\n",
            " 0.99999088 1.00000911 0.99999091 1.00000907 0.99999095 1.00000902\n",
            " 0.99999102 1.00000894 0.9999911  1.00000884 0.99999122 1.00000871\n",
            " 0.99999136 1.00000855 0.99999154 1.00000835 0.99999177 1.0000081\n",
            " 0.99999204 1.0000078  0.99999236 1.00000744 0.99999273 1.00000699\n",
            " 0.99999313 1.0000064  0.99999341 1.0000054  0.99999308 1.00000297\n",
            " 0.99999006 0.99999497 0.99997608 0.9999648  0.9999178  0.99984569\n",
            " 0.99968141 0.99936949 0.99872958 0.99746045 0.99491094 0.98982711\n",
            " 0.97970703 0.95972948 0.92088005 0.84760452]\n",
            "Difference to analytical sol:  0.17792092200380538\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "=================================================================================\n",
            "Optimisation utilisant Gauss-Newton avec recherche du pas Goldstein-Price\n",
            "Iter:  50 | Val:  58.934317399131295 | Step:  0.5  Grad:  67.52998750275327\n",
            "Iter:  100 | Val:  8.906302187462872 | Step:  0.5  Grad:  64.38459873361568\n",
            "\n",
            "Final output:\n",
            "Iter:  106 | Val:  0.0 | Step:  1  Grad:  0.0\n",
            "=================================================================================\n",
            "Number of iterations:  107\n",
            "Final position:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1.]\n",
            "Difference to analytical sol:  0.0\n"
          ]
        }
      ],
      "source": [
        "m1=0.001\n",
        "m2=0.9\n",
        "\n",
        "# Test avec descente de Gradient\n",
        "\n",
        "pp,vv,gg = GDlinesearch(J,GradJ,x0,Tol,Maxiter,m1,m2,v=1)\n",
        "\n",
        "print('Number of iterations: ',len(pp))\n",
        "print('Final position: ',pp[-1])\n",
        "print('Difference to analytical sol: ',np.linalg.norm(analytic-pp[-1]))\n",
        "\n",
        "print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "\n",
        "\n",
        "# Test avec Gauss-Newton\n",
        "pp2,vv2,gg2 = GDlinesearch(J,GradJ,x0,Tol,Maxiter,m1,m2,v=2)\n",
        "\n",
        "print('Number of iterations: ',len(pp2))\n",
        "print('Final position: ',pp2[-1])\n",
        "print('Difference to analytical sol: ',np.linalg.norm(analytic-pp2[-1]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2fccbaa4d782114798190decc59e003c",
          "grade": true,
          "grade_id": "cell-5bf520fd37493b73",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "xSObtjWwdcVN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0d64ef24596ae61c762a63a96fe34d50",
          "grade": true,
          "grade_id": "cell-8f3dc70703df53ee",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "hluUV5JTdcVO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f86815e5da9606e0d1c52270887a3ebe",
          "grade": true,
          "grade_id": "cell-d88520120d0ad0cb",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ddixijeydcVO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4978387405ec1eb431e24dc17fa946c1",
          "grade": true,
          "grade_id": "cell-f72644838216aecb",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Fr_3yb5PdcVP"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fd3784c9ada81be7ca6b0d660c7aa74f",
          "grade": true,
          "grade_id": "cell-967d8fb8839d4a69",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "SXLm8j68dcVQ"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "bd68ad3549c0975da74fec30480c84a9",
          "grade": true,
          "grade_id": "cell-a666ea4f7f8d3709",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "VMmU8paZdcVR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "93e6993abeb9f4e5b7d2cd297cd680b2",
          "grade": true,
          "grade_id": "cell-7fe0bab49eebc293",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "EDdmvVRUdcVS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsVLiY1KdcVS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}